{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import os\n",
    "ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil, sys, csv\n",
    "from pprint import pprint as pp\n",
    "from Bio.PDB import PDBParser, PDBIO, PPBuilder\n",
    "from Bio.SeqUtils import seq1\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.Alphabet import generic_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES Y DATOS GLOBALES\n",
    "\n",
    "path = \"/home/emanuel/Documents/trabajo_actual/MegaScript\"\n",
    "verbose = True\n",
    "data = {}\n",
    "\n",
    "def cargar_txt_en_lista(file_path):\n",
    "    return_list = []\n",
    "    with open(file_path,\"r\") as openfile:\n",
    "        for lines in openfile:\n",
    "            return_list.append(lines.replace(\"\\n\",\"\"))\n",
    "            \n",
    "    return return_list\n",
    "\n",
    "def get_uniprot_from_pdb(pdb_id,chain):\n",
    "    with open(\"archivos_importantes/pdb_chain_uniprot.csv\",\"r\") as openfile:\n",
    "        temp = csv.reader(openfile,delimiter=\",\")\n",
    "        for item in temp:\n",
    "            if (item[0].upper()== pdb_id) and  (item[1]==chain):\n",
    "                return item[2]\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def get_seq_from_uniprot(uniprot):\n",
    "    seqfile = \"fasta/seq_\"+uniprot+\".fasta\"\n",
    "    cmd = \"wget https://www.uniprot.org/uniprot/\"+uniprot+\".fasta -O \"+seqfile\n",
    "    os.system(cmd)\n",
    "    if os.path.isfile:\n",
    "        for entry in SeqIO.parse(seqfile,\"fasta\"):\n",
    "            seq = str(entry.seq)\n",
    "    return seq\n",
    "\n",
    "def bajar_estructura(pdb_id):\n",
    "    os.chdir(path+\"/ent_files/\")\n",
    "    filename =\"pdb\"+pdb_id.lower()+\".ent\"\n",
    "    url = \"ftp://ftp.wwpdb.org/pub/pdb/data/structures/divided/pdb/\"+pdb_id.lower()[1:3]+\"/pdb\"+pdb_id.lower()+\".ent.gz\"\n",
    "    if not os.path.isfile(filename):\n",
    "        #os.system(\"wget ftp://ftp.wwpdb.org/pub/pdb/data/structures/all/pdb/pdb\"+pdb_id.lower()+\".ent.gz\")\n",
    "        wget_code = os.system(\"wget \"+url)\n",
    "        if wget_code == 2048:\n",
    "            os.chdir(path)\n",
    "            return (\"OBSOLETE\")\n",
    "        #Uncompress\n",
    "        os.system(\"gunzip pdb\"+pdb_id.lower()+\".ent.gz\")\n",
    "    else:\n",
    "        if verbose: print (\">> File \"+filename+\" already exists\")\n",
    "    os.chdir(path)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def split_pdb_by_chain(pdb_id):\n",
    "    if not os.path.isdir(\"pdb_chains/\"+pdb_id.upper()):\n",
    "            os.mkdir(\"pdb_chains/\"+pdb_id.upper())\n",
    "    actual_pdbfile = PDBParser().get_structure(pdb_id,\"ent_files/pdb\"+pdb_id.lower()+\".ent\")\n",
    "    return_dict = dict()\n",
    "    for model in actual_pdbfile:\n",
    "        for chain in model:\n",
    "            outfilename = pdb_id.upper() + \"-\" + str(model.get_id()+1) +  \"_\" + str(chain.get_id()) + \".pdb\"\n",
    "            if not os.path.isfile(\"pdb_chains/\"+pdb_id.upper()+\"/\"+outfilename):\n",
    "                io = PDBIO()\n",
    "                io.set_structure(chain)\n",
    "                io.save(\"pdb_chains/\"+pdb_id.upper()+\"/\"+outfilename)\n",
    "            ppb = PPBuilder().build_peptides(chain)\n",
    "            this_seq = Seq(\"\",generic_protein)\n",
    "            for pp in ppb:\n",
    "                 this_seq += pp.get_sequence()\n",
    "            return_dict[outfilename]=this_seq\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "def read_expdata(path_to_pdbfile):\n",
    "    with open(path_to_pdbfile,\"r\") as openfile:\n",
    "        expdata = \"\"\n",
    "        nummodel = 1\n",
    "        for row in openfile:\n",
    "            if \"EXPDTA\" in row:\n",
    "                expdata +=  (row[9:-1].strip())\n",
    "                if \"X-RAY\" in expdata:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            elif \"NUMMDL\" in row:\n",
    "                nummodel = int((row[9:-1].strip()))\n",
    "                break\n",
    "            elif \"REMARK\" in row:\n",
    "                break\n",
    "            elif \"ATOM\" in row:\n",
    "                break\n",
    "    return expdata,nummodel\n",
    "\n",
    "\n",
    "\n",
    "def read_atom_full(pdb_file):\n",
    "    return_lst = []\n",
    "    with open(pdb_file,\"r\") as openfile:\n",
    "        for row in openfile:\n",
    "            if row[0:5] == \"ATOM \":\n",
    "                tmp_lst = []\n",
    "                tmp_lst.append(int(row[6:10+1])) #Integer serial Atom serial number.\n",
    "                tmp_lst.append(row[12:15+1].strip()) #Atom name Atom name.\n",
    "                tmp_lst.append(row[16+1]) #Character altLoc Alternate location indicator.\n",
    "                tmp_lst.append(row[17:19+1]) #Residue name resName Residue name.\n",
    "                tmp_lst.append(row[21+1]) #Character chainID Chain identifier.\n",
    "                tmp_lst.append(int(row[22:25+1])) #Integer resSeq Residue sequence number.\n",
    "                #tmp_lst.append(row[26+1]) #AChar iCode Code for insertion of residues.\n",
    "                #tmp_lst.append(row[30:37+1]) #Real(8.3) x Orthogonal coordinates for X in Angstroms.\n",
    "                #tmp_lst.append(row[38:45+1]) #Real(8.3) y Orthogonal coordinates for Y in Angstroms.\n",
    "                #tmp_lst.append(row[46:53+1]) #Real(8.3) z Orthogonal coordinates for Z in Angstroms.\n",
    "                #tmp_lst.append(row[54:59+1]) #Real(6.2) occupancy Occupancy.\n",
    "                #tmp_lst.append(row[60:65+1]) #Real(6.2) tempFactor Temperature factor.\n",
    "                #tmp_lst.append(row[76:77+1]) #LString(2) element Element symbol, right-justified.\n",
    "                #tmp_lst.append(row[78:89+1].replace(\"\\n\",\"\")) #LString(2) charge Charge on the atom.\n",
    "                return_lst.append(tmp_lst)\n",
    "    return return_lst\n",
    "\n",
    "def print_fixed(query_str,fix=100):\n",
    "    for index,char in enumerate(query_str):\n",
    "        print (char,end=\"\")\n",
    "        if (index+1)%fix ==0:\n",
    "            print (\"\")\n",
    "    print (\"\")\n",
    "    return\n",
    "\n",
    "def save_alidic_to_fasta(ali_dic,outpath):\n",
    "    with open (outpath,\"w\") as openfile:\n",
    "        for itemsin in ali_dic:\n",
    "            openfile.write (\">\"+item+\"\\n\")\n",
    "            openfile.write (ali_dic[item]+\"\\n\")\n",
    "\n",
    "\n",
    "d = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N', \n",
    "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', 'TER':'*',\n",
    "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M','XAA':'X'}\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P15452\n",
      "MKKFLLVAVVGLAGITFANEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent_files already exists...-\n",
      "fasta already exists...-\n",
      "blast already exists...-\n",
      "pdb_chains already exists...-\n",
      ">> File pdb1ayg.ent already exists\n",
      ">> File pdb5aus.ent already exists\n",
      ">> File pdb5aur.ent already exists\n",
      ">> File pdb1ynr.ent already exists\n",
      ">> File pdb4zid.ent already exists\n",
      ">> File pdb3vym.ent already exists\n",
      ">> File pdb2ai5.ent already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 5492.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 5592.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain E is discontinuous at line 5684.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain G is discontinuous at line 5772.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 5858.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 6012.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain E is discontinuous at line 6174.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain G is discontinuous at line 6290.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 3188.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 3260.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 3324.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain D is discontinuous at line 3388.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 3439.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 3487.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 3543.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain D is discontinuous at line 3598.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 2905.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 2991.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 3077.\n",
      "  PDBConstructionWarning)\n",
      "/usr/lib/python3/dist-packages/Bio/PDB/StructureBuilder.py:89: PDBConstructionWarning: WARNING: Chain C is discontinuous at line 3283.\n",
      "  PDBConstructionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P15452\n",
      "MKKFLLVAVVGLAGITFANEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-10_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-11_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-12_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-13_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-14_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-15_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-16_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-17_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-18_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-19_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-1_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-20_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-2_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-3_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-4_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-5_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-6_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-7_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-8_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1AYG-9_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1YNR-1_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1YNR-1_B\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSI-\n",
      "1YNR-1_C\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "1YNR-1_D\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSI-\n",
      "2AI5-10_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-11_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-12_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-13_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-14_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-15_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-16_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-17_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-18_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-19_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-1_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-20_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-2_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-3_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-4_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-5_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-6_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-7_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-8_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "2AI5-9_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "3VYM-1_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "4ZID-1_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "5AUR-1_A\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "5AUR-1_C\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "5AUR-1_E\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "5AUR-1_G\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "5AUS-1_A\n",
      "------------------NEQLAKQKGCMACHDLKA-KVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n",
      "5AUS-1_C\n",
      "------------------NEQLAKQKGCMACHDLKAKKVGPAYADVAKKYAGRKDAVDYLAGKIKKGGSGVWGSVPMPPQNVTDAEAKQLAQWILSIK\n"
     ]
    }
   ],
   "source": [
    "ensemble_name = \"EJEMPLO1\"\n",
    "ensemble = cargar_txt_en_lista(\"archivos_importantes/lista_de_conformeros.txt\")\n",
    "\n",
    "uniprot_id = get_uniprot_from_pdb(ensemble[0][0:4],ensemble[0][-1])\n",
    "uniprot_seq = get_seq_from_uniprot(uniprot_id)\n",
    "\n",
    "#ensemble_name = \"EXAMPLE2\"\n",
    "#ensemble = cargar_txt_en_lista(\"archivos_importantes/ensemble_P37840.txt\")\n",
    "\n",
    "#toma los primeros 4 elemento chars de cada elemento, arma un conjunto\n",
    "pdb_to_download = list(set([element[0:4] for element in ensemble ]))\n",
    "pdb_to_remove = []\n",
    "\n",
    "#Checkquer estructura de carpetas\n",
    "folder_list = [\"ent_files\",\"fasta\",\"blast\",\"pdb_chains\"]\n",
    "for folder in folder_list:\n",
    "    if not os.path.isdir(folder):\n",
    "        print (\"Creating folder \"+ folder)\n",
    "        os.mkdir(folder)\n",
    "    else:\n",
    "        print (folder+\" already exists...-\")\n",
    "\n",
    "#Bajar los enf_files\n",
    "for pdb_id in pdb_to_download:\n",
    "    actual_download = bajar_estructura(pdb_id)\n",
    "    if actual_download == \"OBSOLTE\":\n",
    "        pdb_to_remove.append(pdb_id)\n",
    "        \n",
    "#punto de control, eliminar cosas relacionadas con pdbs obsoletos.\n",
    "ensemble = [items for items in ensemble if items[0:4] not in pdb_to_remove]\n",
    "\n",
    "#le agrega -1 al nombre a aquellas cosas que tienen una unica estructura \n",
    "ensemble = [items if \"-\" in items else (items[0:4]+\"-1_\"+items[-1]) for items in ensemble ]\n",
    "\n",
    "#hay que descartar aquellos pdb que fueron marcados como obsoltos\n",
    "pdb_to_work = list(set(pdb_to_download) - set(pdb_to_remove))\n",
    "\n",
    "#Separar por cadenas y modelos\n",
    "pdb_by_chains = dict()\n",
    "for item in pdb_to_work:\n",
    "    pdb_by_chains[item] = split_pdb_by_chain(item)\n",
    "    \n",
    "#Leer el expdata, cargarla en un diccionario y guardarla en un archivo csv\n",
    "info_expdta = {}\n",
    "filename =\"archivos_importantes/exp_data_\"+ensemble_name+\".csv\" \n",
    "with open(filename,\"w\") as openfile:\n",
    "        for item in pdb_to_work:\n",
    "            temp =read_expdata(\"ent_files/pdb\"+item.lower()+\".ent\")\n",
    "            info_expdta[item] = [temp[0],temp[1]]\n",
    "            openfile.write(item+\",\"+str(temp[0])+\",\"+str(temp[1]))\n",
    "            openfile.write(\"\\n\")  \n",
    "            \n",
    "\n",
    "stru_ali = {}\n",
    "stru_ali[uniprot_id] = uniprot_seq\n",
    "\n",
    "#carga en memoria la informacion importante de sift (requiere pdb_to_work)\n",
    "sifts_data = {}\n",
    "with open(\"archivos_importantes/pdb_chain_uniprot.csv\",\"r\") as openfile:\n",
    "    temp = csv.reader(openfile,delimiter=\",\")\n",
    "    for item in temp:\n",
    "        if len(item) > 2:\n",
    "            pdb_id = item[0].upper()\n",
    "            if pdb_id in pdb_to_work:\n",
    "                pdb_chain =  pdb_id+\"_\"+item[1]\n",
    "                if sifts_data.get(pdb_chain) == None:\n",
    "                    sifts_data[pdb_chain] = [item[3:]]\n",
    "                else:\n",
    "                    sifts_data[pdb_chain].append(item[3:])\n",
    "\n",
    "#la lista se invierte para que sea mas facil mapear\n",
    "for keys in sifts_data:\n",
    "    sifts_data[keys].reverse()\n",
    "    \n",
    "#lee la secuencia de cada archivo, \n",
    "\n",
    "problematic_items = []\n",
    "for item in ensemble:\n",
    "    file_to_read = \"pdb_chains/\"+item[0:4]+\"/\"+item + \".pdb\"\n",
    "    seq_vector = [\"-\"]*len(uniprot_seq)\n",
    "    for n in read_atom_full(file_to_read):\n",
    "        if n[1] == \"CA\":\n",
    "            pos=  int(n[5])-1\n",
    "            aa = d[n[3]]\n",
    "            pdb_chain = item[0:4]+\"_\"+item[-1]\n",
    "            \n",
    "            #print (pdb_chain,pos)\n",
    "            for sdata in sifts_data[pdb_chain]:\n",
    "                if pos >= int(sdata[0])-1:\n",
    "                    val1 = int(sdata[0])-1\n",
    "                    val2 = int(sdata[4])-1\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if val2 > val1:\n",
    "                new_pos = pos - val1 + val2\n",
    "            elif val2 == val1:\n",
    "                new_pos = pos - val1\n",
    "                \n",
    "            if new_pos < len(uniprot_seq):\n",
    "                seq_vector[new_pos] = aa\n",
    "            else:\n",
    "                print (\"ERROR\",item,aa,pos,val1,val2,new_pos)\n",
    "                problematic_items.append(item)\n",
    "                break\n",
    "    seq = \"\"\n",
    "    for i in seq_vector:\n",
    "        seq += i\n",
    "    stru_ali[item] = seq\n",
    "\n",
    "for n in stru_ali:\n",
    "    print (n)\n",
    "    print_fixed (stru_ali.get(n))\n",
    "\n",
    "\n",
    "#Guarda el alineamiento en el disco\n",
    "save_alidic_to_fasta(stru_ali,\"fasta/\"+ensemble_name+\"_stru_ali.fasta\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alieneamiento de homologos y arbol\n",
    "\n",
    "\n",
    "def run_blast(fasta_file):\n",
    "    output_filename = \"blast_results/\"+uniprot_id+\"_blast_results.txt\"\n",
    "    cmd = \"blastp -query \"+ fasta_file +\" -db uniprot_sprot.db -evalue 1e-10 -out \"+ output_filename +\" -outfmt '6 qaccver qlen qstart qend sseqid saccver slen sstart send length staxid nident gaps evalue pident qcovs' -num_alignments 1000\"\n",
    "    os.system(cmd)\n",
    "    return output_filename\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "resultados_blast = run_blast(\"fasta/seq_\"+uniprot_id+\".fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qaccver', 'qlen', 'qstart', 'qend', 'sseqid', 'saccver', 'slen', 'sstart', 'send', 'length', 'staxid', 'nident', 'gaps', 'evalue', 'pident', 'qcovs']\n",
      "['sp|P15452|CY552_HYDTT', '98', '1', '98', 'sp|P15452|CY552_HYDTT', 'sp|P15452|CY552_HYDTT', '98', '1', '98', '98', 'N/A', '98', '0', '2.91e-66', '100.000', '100\\n']\n"
     ]
    }
   ],
   "source": [
    "a =  \"qaccver qlen qstart qend sseqid saccver slen sstart send length staxid nident gaps evalue pident qcovs\".split(\" \")\n",
    "\n",
    "with open(resultados_blast, \"r\") as openfile:\n",
    "    for lines in openfile:\n",
    "        print (lines.split(\"\\t\"))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_output_blast(blast_output):\n",
    "  # Descartar lo que cumpla con la condicion 35 idetnidad y 70 de coverage\n",
    "    if verbose: print (\"> Applying filters to identity and coverage...\")\n",
    "    output_filename1 = blast_output.replace(\"blast_output\",\"blast_out_filtered\")\n",
    "    output_filename2 = blast_output.replace(\"blast_output\",\"blast_out_filtered_300\")\n",
    "    dic_ident = dict()\n",
    "    for n in range(3,11):\n",
    "        dic_ident[n] =list()\n",
    "    lista_iden = list()\n",
    "    lista_acc = list()\n",
    "    with open(blast_output,\"r\") as inputfile:\n",
    "        for line in inputfile:\n",
    "            a = line.split()\n",
    "            identity = float(a[14])\n",
    "            coverage = float(a[15])\n",
    "            accession = a[4]\n",
    "            if (identity > 35) and (identity < 100) and (coverage > 70):\n",
    "              #deberia ser menor estricto a 100, porque si es 100% identica no estas agarrando la misma?\n",
    "                with open(output_filename1, 'a') as out:\n",
    "                    out.write(line)\n",
    "                coef = int(identity/10)\n",
    "                lista_iden.append(coef)\n",
    "                tupla_t = ( accession.split(\"|\")[1],identity)\n",
    "                dic_ident[coef].append(tupla_t)\n",
    "                lista_acc.append(accession.split(\"|\")[1])\n",
    "\n",
    "    borrar=False\n",
    "    #Esto printea una suerte de histograma de % de identidad.\n",
    "    if len(lista_iden)< 30:\n",
    "        warning_message = \"WARNING: filtered blast with less that 30 results(\"+str(len(lista_iden))+\" results)\"\n",
    "        prRed (warning_message)\n",
    "        borrar=True\n",
    "        #Esta variable indica que no se debe seguir trabajando con estos datos por generar muy pocos resultados de blast.\n",
    "    if len(lista_iden)> 300:\n",
    "        warning_message = \"WARNING: filtered blast with more that 300 results(\"+str(len(lista_iden))+\" results)\"\n",
    "        prRed (warning_message)\n",
    "\n",
    "    final_acc_list = select_if_mas_de_300(dic_ident)\n",
    "\n",
    "    if verbose: print (\"> Filtred blast results: \"+output_filename1)\n",
    "    return output_filename1, final_acc_list, borrar\n",
    "\n",
    "def select_if_mas_de_300 (dic_ident):\n",
    "    print (\"HISTOGRAMA==\")\n",
    "    new_dict = dict()\n",
    "    for n in range(3,11):\n",
    "        dic_ident[n].sort() # Ordena el primer el\n",
    "        if len(dic_ident[n])>20:\n",
    "            print (n*10,len(dic_ident[n]))\n",
    "            new_dict[n] = dic_ident[n][0:20]\n",
    "        else:\n",
    "            print (n*10,len(dic_ident[n]))\n",
    "            new_dict[n] = dic_ident[n]\n",
    "    final_acc_list = list()\n",
    "    for ranges in new_dict:\n",
    "        for items in new_dict[ranges]:\n",
    "            final_acc_list.append(items[0])\n",
    "    return final_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nota: base de dato de mapeo de residuos pdb/uniprot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
